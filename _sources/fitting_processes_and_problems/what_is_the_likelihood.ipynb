{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4050b3a7-e615-4418-ab31-0c4c90bd70e7",
   "metadata": {},
   "source": [
    "# What is the likelihood?\n",
    "\n",
    "So far we have looked at how we can define, and reparameterise, our model. \n",
    "Now, we should consider how we **compare** our model to some experimental data. \n",
    "The broad aim of *fitting* is to get the model data to agree as best as is possible to the experimental data.\n",
    "Here, we discuss fitting in terms of the maximisation of likelihood, this is analogous to the [minimisation](https://en.wikipedia.org/wiki/Minimum_chi-square_estimation) of a $\\chi^2$ goodness-of-fit metric.\n",
    "\n",
    "The likelihood, $\\mathcal{L}(\\theta|y)$, is a measure of the agreement between the model, which depends on parameters $\\theta$, given some data, $y$.\n",
    "If we consider some values for $y$, which increase linearly with $x$ and have associated with them some value for $\\sigma$, the uncertainty in $y$. \n",
    "Typically, we plot these uncertainties as **error bars** on our data. \n",
    "However, these uncertainties typically indicate a single standard deviation for the underlying probability distribution that the observed values of $y$ are selected from. \n",
    "This is represented by the blue points and error bars and probability distributions shown in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b279a-4d37-46dd-b0f6-a44cec52906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import norm\n",
    "\n",
    "def straight_line(x: np.ndarray, m:float, c:float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param x: the abscissa data\n",
    "    :param m: gradient of line\n",
    "    :param c: intercept\n",
    "    :returns: model values\n",
    "    \"\"\"\n",
    "    return m * x + c\n",
    "\n",
    "x, y, sigma = np.loadtxt('data.txt')\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12.8, 4.8))\n",
    "m, c = curve_fit(straight_line, x, y, sigma=sigma)[0]\n",
    "ax[0].errorbar(x, y, sigma, marker='o', ls='')\n",
    "ax[0].plot(x, straight_line(x, m, c), ls='-', zorder=10)\n",
    "ax[0].set_xlabel('$x$')\n",
    "ax[0].set_ylabel('$y$')\n",
    "yy = np.linspace(-7, 42, 1000)\n",
    "y_fit = straight_line(x, m, c)\n",
    "for i, d in enumerate(y):\n",
    "    ax[1].fill_between(yy, 0, norm(d, sigma[i]).pdf(yy), alpha=(i + 1) * 0.1, color='#1f77b4', lw=0)\n",
    "    ax[1].plot(y_fit[i], norm(d, sigma[i]).pdf(y_fit[i]), marker='o', color='#ff7f0e')\n",
    "ax[1].set_ylabel('$p(y)$')\n",
    "ax[1].set_xlabel('$y$')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf574c11-9bc1-4a7a-9653-089bdf38018e",
   "metadata": {},
   "source": [
    "When we perform model-dependent analysis, we aim to find the parameters for some model that find the overall maximum for these probability distributions. \n",
    "This can be seen for the *line of best fit* to the data above, where the orange dots show where on these probability distributions this maximum likelihood model sits. \n",
    "For data where the uncertainty represents a single standard deviation on a normal distribution, it is possible to calculate the likelihood, or as is done more commonly the log-likelihood, for a given model, $\\mathrm{M}$, with the following, \n",
    "\n",
    "$$\n",
    "\\ln[\\mathcal{L}(\\theta | y)] = -\\frac{1}{2}\\sum_{n=1}^{n_{\\mathrm{max}}}\\bigg\\{\\frac{[y_n - \\mathrm{M}(x_n, \\theta)] ^ 2}{\\sigma_n ^ 2} + \\ln(2\\pi \\sigma_n  ^ 2)\\bigg\\},\n",
    "$$\n",
    "\n",
    "where $n_{\\mathrm{max}}$ is the number of datapoints. \n",
    "You may notice that the first part of the summation is the same as a typical $\\chi^2$ goodness-of-fit metric that one may minimise, with the other elements being normalisation factors. \n",
    "For data where the uncertainty is not normally distributed, other likelihood functions may be used, however, the majority of reflectometry analysis considers normal uncertainties.\n",
    "\n",
    "The value of a likelihood (and any goodness of fit metric) depends heavily on the data that it is describing. \n",
    "Hence, the likelihood is written $\\mathcal{L}(\\theta | y)$, where the $|y$ indicates that this is given some data $y$. \n",
    "This means that **one cannot compare likelihood values across datasets**, it is only possible to compare likelihood values across models for a single dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
